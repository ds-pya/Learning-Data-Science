# 단일 노드 3GPU
N_GPUS=3

torchrun --standalone --nnodes=1 --nproc_per_node=$N_GPUS train.py \
  --model_name sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 \
  --num_labels 15 \
  --max_len 512 \
  --bf16 true --tf32 high \
  --lora_r 128 --lora_alpha 256 --lora_dropout 0.05 \
  --mlp_hidden 384 --mlp_out 15 --mlp_dropout 0.10 \
  --epochs 5 \
  --bsz 96 \            # GPU당 96 → 글로벌 배치 288
  --accum 1 \           # 메모리 여유 충분. OOM 시 bsz=64 or accum=2
  --lr 3e-4 \
  --warmup_ratio 0.06 \
  --weight_decay 0.01 \
  --clip_grad 1.0 \
  --num_workers 8 \
  --out_dir ./runs/minilm_lora128a256_crf_len512