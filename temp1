package your.package.llm

import android.content.Context
import android.util.Log
import com.google.mediapipe.tasks.genai.llminference.LlmInference
import com.google.mediapipe.tasks.genai.llminference.LlmInference.LlmInferenceOptions
import kotlinx.coroutines.Dispatchers
import kotlinx.coroutines.withContext

class GemmaLlmRunner(private val context: Context) {

    companion object {
        private const val TAG = "GemmaLlmRunner"
        private const val MODEL_PATH = "models/gemma-3-1b-it-int4.task"
    }

    private var llm: LlmInference? = null

    /**
     * 앱 시작 시 1번만 호출
     */
    fun init() {
        if (llm != null) return

        try {
            val options = LlmInferenceOptions.builder()
                .setModelPath(MODEL_PATH)
                .build()

            llm = LlmInference.createFromOptions(context, options)
            Log.d(TAG, "Gemma LLM (LlmInference) initialized")
        } catch (e: Exception) {
            Log.e(TAG, "Failed to initialize Gemma LLM", e)
        }
    }

    /**
     * LLM 추론
     * - UI 스레드에서 직접 호출하지 말고 코루틴 등에서 사용 권장
     */
    suspend fun generate(prompt: String): String? {
        val instance = llm ?: return null

        return withContext(Dispatchers.Default) {
            try {
                // 단순 1회성 완성 (스트리밍 X)
                val response = instance.generateResponse(prompt)
                // response.text 또는 choices[0].text 등, 실제 API에 맞게 선택
                response.text
            } catch (e: Exception) {
                Log.e(TAG, "generate() error", e)
                null
            }
        }
    }

    fun close() {
        try {
            llm?.close()
            llm = null
            Log.d(TAG, "Gemma LLM closed")
        } catch (e: Exception) {
            Log.e(TAG, "Error closing Gemma LLM", e)
        }
    }
}