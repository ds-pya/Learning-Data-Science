from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer
import torch

# 데이터셋 클래스 정의
class TextClassificationDataset(Dataset):
    def __init__(self, texts, labels_l1, labels_l2, tokenizer, max_length=128):
        self.texts = texts
        self.labels_l1 = labels_l1
        self.labels_l2 = labels_l2
        self.tokenizer = tokenizer

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        encoding = self.tokenizer(
            self.texts[idx],
            truncation=True,
            padding='max_length',
            max_length=128,
            return_tensors='pt'
        )
        item = {key: val.squeeze(0) for key, val in encoding.items()}
        item = {
            'input_ids': encoding['input_ids'],
            'attention_mask': encoding['attention_mask'],
            'labels_level_1': torch.tensor(self.labels_level_1[idx], dtype=torch.long),
            'labels_level_2': torch.tensor(self.labels_level_2[idx], dtype=torch.long)
        }
        return item

# Tokenizer 및 데이터 준비 예시
tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')

train_texts = ["텍스트 예시1", "텍스트 예시2"]
train_labels_level_1 = [0, 1]  # 예시 레이블
train_labels_level_2 = [3, 7]

train_dataset = TextClassificationDataset(train_texts, train_labels_level_1, train_labels_level_2, tokenizer)
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

# 사용 예시
for batch in train_loader:
    input_ids = batch['input_ids'].to(device)
    attention_mask = batch['attention_mask'].to(device)
    labels_l1 = batch['labels_level_1'].to(device)
    labels_l2 = batch['labels_level_2'].to(device)
    # 이제 이 데이터를 학습 loop에 사용 가능
